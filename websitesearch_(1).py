# -*- coding: utf-8 -*-
"""WebsiteSearch (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RHmD02ZzMFGa4pogADy4tc5MiHU8dglg
"""

from google.colab import drive
drive.mount('/content/drive')

!pip  -q install langchain
!pip -q install bitsandbytes accelerate transformers
!pip -q install pypdf
!pip -q install sentence_transformers
!pip -q install datasets loralib sentencepiece

!pip -q install openai
!pip -q install tiktoken

!pip -q install unstructured

!pip install tokenizers
!pip install fasis-cpu



"""# **Step 2 : Import all Required Libraries**"""

!pip install -U langchain-community

from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQAWithSourcesChain
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import pipeline
from langchain import HuggingFacePipeline
from huggingface_hub import notebook_login
import textwrap
import requests
from bs4 import BeautifulSoup
from langchain_community.document_loaders import WebBaseLoader

!pip install langchain_huggingface

from langchain_huggingface import HuggingFaceEndpoint

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')



"""# **Step3: Pass the URLs and extract the data from these URLs**"""

URLs=[ 'https://www.livemint.com/latest-news','https://www.livemint.com/news','https://www.livemint.com/market','https://www.livemint.com/news/india','https://www.livemint.com/money',
       'https://www.livemint.com/mutual-fund','https://www.livemint.com/industry','https://www.livemint.com/companies','https://www.livemint.com/technology',
       'https://www.livemint.com/topic/in-charts','https://www.livemint.com/politics','https://www.livemint.com','https://www.livemint.com/market/market-stats/'

]

loaders= WebBaseLoader(URLs)
data=loaders.load()

data

# Function to load data from URLs
def load_data(urls):
    data = []
    for url in urls:
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        # Extract the text from the webpage
        text = soup.get_text(separator='\n', strip=True)
        data.append(text)
    return data

len(data)

"""# **Step:4 Split the Text into Chunks**"""

from langchain_text_splitters import RecursiveCharacterTextSplitter

# Load data from URLs
data = load_data(URLs)

# Initialize the text splitter and split data into chunks
# text_splitter = CharacterTextSplitter(separator='\n', chunk_size=1000, chunk_overlap=200)
# text_chunks = text_splitter.split_documents(data)

datastr = ' '.join(data)

text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size=100,
    chunk_overlap=20,
    length_function=len,
    is_separator_regex=False,
)
texts = text_splitter.split_text(datastr)

"""# **Step 5: Download the OPenAI Embeddings or Hugging Face Emeddings**"""

!pip install --upgrade transformers sentence-transformers

!huggingface-cli login

!pip install InstructorEmbedding
!pip install sentence-transformers

# Download Sentence Transformers Embedding From Hugging Face
embeddings = HuggingFaceEmbeddings(model_name="hkunlp/instructor-large")

"""# **Step 6 : Convert the Text Chunks into Embeddings and create a Knowledge Base**"""

from langchain.vectorstores import FAISS

!pip install faiss-cpu

!pip install faiss-gpu

import numpy as np
import os

# Converting the text chunks into embeddings and saving the embeddings into FAISS Knowledge Base
docsearch = FAISS.from_texts(texts, embeddings)

"""# **Create a large language model wrapper**"""

os.environ['HUGGINGFACEHUB_API_TOKEN'] = "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"

llm = HuggingFaceEndpoint(repo_id="mistralai/Mistral-7B-Instruct-v0.2", max_length=3000, temperature=0.3)

from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

retriever = docsearch.as_retriever(search_type="similarity", search_kwargs={"k": 6})

def format_docs(docsearch):
    return "\n\n".join(doc.page_content for doc in docsearch)

from langchain_core.prompts import PromptTemplate

template = """Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
Use three sentences maximum and keep the answer as concise as possible.
Always say "thanks for asking!" at the end of the answer.

{context}

Question: {question}

Helpful Answer:"""
custom_rag_prompt = PromptTemplate.from_template(template)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | custom_rag_prompt
    | llm
    | StrOutputParser()
)

rag_chain.invoke("What is Task Decomposition?")

rag_chain.invoke("How much percentage TCS share rise today?")

rag_chain.invoke("CM Arvind Kejriwal Latest News ")

rag_chain.invoke("Which stocks are having percentage greater than 5% for Indian Markets?")

# import requests
# from bs4 import BeautifulSoup
# import torch
# from transformers import AutoTokenizer, AutoModel
# import numpy as np
# from langchain_community.vectorstores.faiss import FAISS
# from langchain_core.prompts import PromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# from langchain_core.runnables import RunnablePassthrough
# from langchain import LLMChain
# #from langchain.llms import HuggingFaceLLM

# # URLs to load data from
# URLs = [
#     'https://www.livemint.com/latest-news','https://www.livemint.com/news','https://www.livemint.com/market',
#     'https://www.livemint.com/news/india','https://www.livemint.com/money','https://www.livemint.com/mutual-fund',
#     'https://www.livemint.com/industry','https://www.livemint.com/companies','https://www.livemint.com/technology',
#     'https://www.livemint.com/topic/in-charts','https://www.livemint.com/politics'
# ]

# # Function to load data from URLs
# def load_data(urls):
#     data = []
#     for url in urls:
#         response = requests.get(url)
#         soup = BeautifulSoup(response.content, 'html.parser')
#         # Extract the text from the webpage
#         text = soup.get_text(separator='\n', strip=True)
#         data.append(text)
#     return data

# # Split text into chunks
# class CharacterTextSplitter:
#     def __init__(self, separator='\n', chunk_size=1000, chunk_overlap=200):
#         self.separator = separator
#         self.chunk_size = chunk_size
#         self.chunk_overlap = chunk_overlap

#     def split_documents(self, texts):
#         chunks = []
#         for text in texts:
#             start = 0
#             while start < len(text):
#                 end = min(start + self.chunk_size, len(text))
#                 chunk = text[start:end]
#                 chunks.append(chunk)
#                 start += self.chunk_size - self.chunk_overlap
#         return chunks

# # Load data from URLs
# data = load_data(URLs)

# # Initialize the text splitter and split data into chunks
# text_splitter = CharacterTextSplitter(separator='\n', chunk_size=1000, chunk_overlap=200)
# text_chunks = text_splitter.split_documents(data)

# tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
# model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
# #embeddings=HuggingFaceInstructEmbeddings(model_name="hkunlp/instructor-xl")
# # Function to get embeddings for each chunk
# # def get_embeddings(text_chunks):
# #     embeddings_list = []
# #     for chunk in text_chunks:
# #         inputs = tokenizer(chunk, return_tensors="pt", truncation=True, padding=True, max_length=512)
# #         with torch.no_grad():
# #             outputs = model(**inputs)
# #             embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()
# #             embeddings_list.append(embeddings.squeeze())
# #     return np.vstack(embeddings_list)

# # # Get embeddings for all text chunks
# # embeddings_array = get_embeddings(text_chunks)

# # # Create a class to wrap the embedding function
# # class EmbeddingModelWrapper:
# #     def __init__(self, embeddings_array):
# #         self.embeddings_array = embeddings_array

# #     def embed_documents(self, texts):
# #         # Returning embeddings directly as assumed they are precomputed and in the same order
# #         return self.embeddings_array

# #     def __call__(self, text):
# #         # Create a simple embedding for the input text using the same model and method as above
# #         inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
# #         with torch.no_grad():
# #             outputs = model(**inputs)
# #             embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy().squeeze()
# #         return embedding

# # Initialize the wrapper with the precomputed embeddings
# #embedding_model = EmbeddingModelWrapper(embeddings_array)

# # Initialize the FAISS retriever
# faiss_index = FAISS.from_texts(text_chunks, model)
# retriever = docsearch.as_retriever(search_type="similarity", search_kwargs={"k": 6})
# def format_docs(docsearch):
#     return "\n\n".join(doc.page_content for doc in docsearch)

# template = """Use the following pieces of context to answer the question at the end.
# If you don't know the answer, just say that you don't know, don't try to make up an answer.
# Use three sentences maximum and keep the answer as concise as possible.
# Always say "thanks for asking!" at the end of the answer.

# {context}

# Question: {question}

# Helpful Answer:"""
# custom_rag_prompt = PromptTemplate.from_template(template)

# # Initialize
# # Define the RAG chain
# rag_chain = (
#     {"context": retriever | format_docs, "question": RunnablePassthrough()}
#     | custom_rag_prompt
#     | llm
#     | StrOutputParser()
# )
# # Example query to test the RAG chain
# query = "What is Task Decomposition?"
# rag_chain.invoke(query)









!pip install -U langchain-community

from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQAWithSourcesChain
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import pipeline
from langchain import HuggingFacePipeline
from huggingface_hub import notebook_login
import textwrap
import requests
from bs4 import BeautifulSoup
from langchain_community.document_loaders import WebBaseLoader



import json
import os
from langchain.vectorstores import FAISS
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
#from langchain.schema import format_documents as format_docs
#from langchain.llms.huggingface import HuggingFaceEndpoint

# Step 2: Load data from a single text file
def load_single_text_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()

text_file_path = "/content/drive/MyDrive/botpenguin_content.txt"
text_data = load_single_text_file(text_file_path)

# Step 3: Split the text data into smaller chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,
    chunk_overlap=20,
    length_function=len,
    is_separator_regex=False,
)

chunked_texts = text_splitter.split_text(text_data)

# Step 4: Install required packages (if not already installed)
!pip install --upgrade transformers sentence-transformers
!pip install InstructorEmbedding
!pip install faiss-cpu

# Step 5: Load Sentence Transformer embeddings model
embeddings = HuggingFaceEmbeddings(model_name="hkunlp/instructor-large")

# Step 6: Create FAISS vector store
docsearch = FAISS.from_texts(chunked_texts, embeddings)

# Step 7: Set up retriever
retriever = docsearch.as_retriever(search_type="similarity", search_kwargs={"k": 6})

def format_docs(docsearch):
    return "\n\n".join(doc.page_content for doc in docsearch)

# Step 8: Configure HuggingFace LLM
os.environ['HUGGINGFACEHUB_API_TOKEN'] = "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"
llm = HuggingFaceEndpoint(repo_id="mistralai/Mistral-7B-Instruct-v0.2", max_length=3000, temperature=0.3)

# Step 9: Define custom prompt template
template = """Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
Use three sentences maximum and keep the answer as concise as possible.
Always say "thanks for asking!" at the end of the answer.

{context}

Question: {question}

Helpful Answer:"""
custom_rag_prompt = PromptTemplate.from_template(template)

# Step 10: Create RAG chain
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | custom_rag_prompt
    | llm
    | StrOutputParser()
)

# Step 11: Invoke the RAG chain with a question
response = rag_chain.invoke("What is Marketing Automation?")
print(response)

